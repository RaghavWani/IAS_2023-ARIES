{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f441c6",
   "metadata": {},
   "source": [
    "# **Aperture photometry**\n",
    "\n",
    "\n",
    "_**Developed by**_  <br>\n",
    "Raghav Wani <br>\n",
    "Vivek Jha\n",
    "\n",
    "_**Under guidance of**_ <br>\n",
    "Dr. Yogesh Joshi <br>\n",
    "Aryabhatta Research Institute of Observational Sciences (ARIES), Nainital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import pandas as pd\n",
    "import math, re, random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import ccdproc,os,sys,time,random, csv, os\n",
    "import warnings\n",
    "import astroalign as aa\n",
    "from glob import glob\n",
    "from itertools import combinations\n",
    "#ASTROPY\n",
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from astropy.wcs import WCS\n",
    "from astropy.nddata import CCDData\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "from astropy import coordinates as coord\n",
    "from astropy.stats import sigma_clipped_stats, SigmaClip\n",
    "from astropy.visualization import ImageNormalize, LogStretch, ZScaleInterval, SqrtStretch, simple_norm\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize\n",
    "from astropy.stats import SigmaClip, mad_std\n",
    "from astroquery.simbad import Simbad\n",
    "#PHOTUTILS\n",
    "from photutils.datasets import make_100gaussians_image\n",
    "from photutils.background import Background2D, MeanBackground,SExtractorBackground\n",
    "from photutils import find_peaks, CircularAperture, CircularAnnulus, aperture_photometry\n",
    "from photutils import Background2D, MedianBackground, DAOStarFinder\n",
    "from photutils.utils import calc_total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344baf0",
   "metadata": {},
   "source": [
    "#### **Folder name**\n",
    "folder where all the images are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = '/home/aries/projetFolder/NGC6709/NGC6709_1/20211009/' #should start and end with \"/\"\n",
    "warnings.filterwarnings(\"ignore\") #ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4868ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check details of the frame\n",
    "files=sorted(glob.glob(os.path.join(tmp_dir, 'S-*cleaned.fits'))) #list of science frame path. \n",
    "print(len(files))\n",
    "for i in range(len(files)):\n",
    "    data_0,header_0=fits.getdata(files[i],header=True)\n",
    "    print(header_0['RA'], header_0['DEC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef014f",
   "metadata": {},
   "source": [
    "####  **Clearing unwanted data from the night of observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0cf51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(tmp_dir)\n",
    "\n",
    "start = time.time()\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(tmp_dir, file_name)\n",
    "\n",
    "    #change starts with as per target file:\n",
    "    if file_name.endswith('.fits') and file_name.startswith('F-2022'): \n",
    "        \n",
    "        with fits.open(file_path) as hdul:\n",
    "            header = hdul[0].header\n",
    "        # keyword used for sorting:\n",
    "        object_value = header.get('FILTER1') #change as per target file\n",
    "        #for S- use: ORIGFILE\n",
    "        #for F- use: FILTER1\n",
    "\n",
    "        # Delete the file \n",
    "        if not object_value.startswith('R'): #might change as per header\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted file: {file_name}\")\n",
    "            \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa7c56",
   "metadata": {},
   "source": [
    "### **Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a930ab",
   "metadata": {},
   "source": [
    "###### A. Creating Master bias frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95564cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bias_files = sorted(glob(os.path.join(tmp_dir, \"BIAS*.fits\")))\n",
    "biaslist = []\n",
    "print(bias_files)\n",
    "for i in range (len(bias_files)):\n",
    "    data= ccdproc.CCDData.read(bias_files[i],unit='adu')\n",
    "    print(data.data.shape)\n",
    "    biaslist.append(data)\n",
    "    \n",
    "masterbias = ccdproc.combine(biaslist,method='median',sigma_clip=True, sigma_clip_low_thresh=5, sigma_clip_high_thresh=5,\n",
    "                             sigma_clip_func=np.ma.median, sigma_clip_dev_func=mad_std)\n",
    "\n",
    "masterbias.write(tmp_dir+ 'masterbias.fits', overwrite=True)\n",
    "\n",
    "mbias=ccdproc.CCDData.read(tmp_dir + 'masterbias.fits', unit='adu')\n",
    "print('Master bias generated')\n",
    "print(\" Mean and median of the masterbias:\", np.mean(masterbias), np.median(masterbias))\n",
    "\n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c30f9",
   "metadata": {},
   "source": [
    "###### B. Creating Master flat frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0ddd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "flat_files=sorted(glob(os.path.join(tmp_dir, \"F-*.fits\")))\n",
    "flatlist=[]\n",
    "for j in range(len(flat_files)):\n",
    "    flat=ccdproc.CCDData.read(flat_files[j],unit='adu')\n",
    "    mbias=ccdproc.CCDData.read(tmp_dir + 'masterbias.fits',unit='adu')\n",
    "    print(mbias.data.shape)\n",
    "    print(flat.data.shape)\n",
    "    flat_bias_removed=ccdproc.subtract_bias(flat,mbias) #flat - master bias\n",
    "    flatlist.append(flat_bias_removed)\n",
    "\n",
    "# normalizing masterflat:\n",
    "def inv_median(a):\n",
    "    return 1 / np.median(a)\n",
    "\n",
    "masterflat = ccdproc.combine(flatlist,method='median',scale=inv_median ,sigma_clip=True, sigma_clip_low_thresh=5, sigma_clip_high_thresh=5, sigma_clip_func=np.ma.median, sigma_clip_dev_func=mad_std)\n",
    "masterflat.write(tmp_dir+ 'masterflat.fits', overwrite=True)\n",
    "mflat=ccdproc.CCDData.read(tmp_dir + 'masterflat.fits',unit='adu')\n",
    "print('Master flat generated')\n",
    "print(\"Mean and median of the masterflat: \", np.mean(masterflat), np.median(masterflat))\n",
    "\n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577bf26",
   "metadata": {},
   "source": [
    "###### C. Master bias substraction, flat-fielding, cosmis rays removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d1c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FINAL CODE FOR CLEANING:\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files=sorted(glob(os.path.join(tmp_dir, \"S-*.fits\")))\n",
    "for i in range(len(files)): #(0,len(files))\n",
    "    print(files[i])\n",
    "    image=ccdproc.CCDData.read(files[i],unit='adu')\n",
    "    header=fits.getheader(files[i],0)\n",
    "    \n",
    "    mbias=ccdproc.CCDData.read(tmp_dir+ 'masterbias.fits',unit='adu') \n",
    "    mflat=ccdproc.CCDData.read(tmp_dir+ 'masterflat.fits',unit='adu')\n",
    "    #shape should be 2D, if not use '[0]'\n",
    "    print(mbias.data.shape) #(2048, 2048)\n",
    "    print(mflat.data.shape) #(2048, 2048)\n",
    "    print(image.data.shape) #(2048, 2048)\n",
    "    \n",
    "    if image.data.shape == mbias.data.shape: #to avoid kinetic mode data\n",
    "        bias_subtracted = ccdproc.subtract_bias(image, mbias)\n",
    "        flat_corrected = ccdproc.flat_correct(bias_subtracted, mflat)\n",
    "        \n",
    "        cr_cleaned = ccdproc.cosmicray_lacosmic(flat_corrected,readnoise=10, sigclip=5, verbose=True)\n",
    "        print('Cosmic rays removed')\n",
    "        clean_file=files[i].replace('.fits','')\n",
    "    \n",
    "        fits.writeto(clean_file+'_cleaned.fits',cr_cleaned, header=header,overwrite=True)\n",
    "        print('Image no',i+1,'has been cleaned')\n",
    "        print(\" Mean and median of the cleaned image: \",np.mean(cr_cleaned), np.median(cr_cleaned))  \n",
    "\n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e09322",
   "metadata": {},
   "source": [
    "###### D. Plotting cleaned images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cf5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of science frame\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tmp_dir = tmp_dir = '/home/aries/projetFolder/NGC6709/NGC6709_2/20211105/'\n",
    "files = sorted(glob.glob(os.path.join(tmp_dir, \"S-*.fits\")))\n",
    "print(files[0])\n",
    "\n",
    "\n",
    "hdul = fits.open(files[0])\n",
    "image_data = hdul[0].data\n",
    "wcs = WCS(hdul[0].header)\n",
    "\n",
    "# Display the image with reverse gray scale\n",
    "interval = ZScaleInterval()\n",
    "vmin, vmax = interval.get_limits(image_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=wcs)\n",
    "\n",
    "im = ax.imshow(image_data, cmap='gray_r', origin='lower', vmin=vmin, vmax=vmax)\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Right ascension')\n",
    "ax.set_ylabel('Declination')\n",
    "\n",
    "ax.tick_params(axis='both', direction='in', width=1.5, length=5, labelsize=8)#, top=True, left=True, right=True)\n",
    "\n",
    "x_ticks_pix = ax.get_xticks()\n",
    "y_ticks_pix = ax.get_yticks()\n",
    "\n",
    "x_ticks_world, y_ticks_world = wcs.wcs_pix2world(x_ticks_pix, y_ticks_pix, 0)\n",
    "\n",
    "ra_ticks = Angle(x_ticks_world, unit='deg').to_string(unit=u.hourangle, sep=':')\n",
    "ax.set_xticklabels(ra_ticks, fontweight='bold')\n",
    "\n",
    "dec_ticks = Angle(y_ticks_world, unit='deg').to_string(unit=u.degree, sep=':')\n",
    "ax.set_yticklabels(dec_ticks, fontweight='bold')\n",
    "plt.title(\"NGC6709 Uncleaned image\")\n",
    "plt.show()\n",
    "hdul.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of Flat and Bias frames\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tmp_dir = '/home/aries/projetFolder/NGC6709/NGC6709_2/20211105/'\n",
    "\n",
    "files = sorted(glob(os.path.join(tmp_dir, \"F-*.fits\")))\n",
    "print(files[0])\n",
    "hdul = fits.open(files[0])\n",
    "image_data = hdul[0].data\n",
    "wcs = WCS(hdul[0].header)\n",
    "\n",
    "interval = ZScaleInterval()\n",
    "vmin, vmax = interval.get_limits(image_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=wcs)\n",
    "\n",
    "im = ax.imshow(image_data, cmap='gray', origin='lower', vmin=vmin, vmax=vmax)\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "\n",
    "ax.set_xlabel('X axis pixel')\n",
    "ax.set_ylabel('Y axis pixel')\n",
    "\n",
    "ax.tick_params(axis='both', direction='in', width=1.5, length=5, labelsize=8)#, top=True, left=True, right=True)\n",
    "\n",
    "x_ticks_pix = ax.get_xticks()\n",
    "y_ticks_pix = ax.get_yticks()\n",
    "\n",
    "x_ticks_world, y_ticks_world = wcs.wcs_pix2world(x_ticks_pix, y_ticks_pix, 0)\n",
    "\n",
    "ra_ticks = Angle(x_ticks_world, unit='deg').to_string(unit=u.hourangle, sep=':')\n",
    "ax.set_xticklabels(ra_ticks, fontweight='bold')\n",
    "\n",
    "dec_ticks = Angle(y_ticks_world, unit='deg').to_string(unit=u.degree, sep=':')\n",
    "ax.set_yticklabels(dec_ticks, fontweight='bold')\n",
    "plt.title(\"Flat frame\")\n",
    "plt.show()\n",
    "hdul.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9b296",
   "metadata": {},
   "source": [
    "### **Aperture photometry**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f7cf7",
   "metadata": {},
   "source": [
    "###### A. Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a774a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For source detection:\n",
    "#'files' is the list of path of all the cleaned frames.\n",
    "# 'n' is the index number of particular cleaned frame in 'files'\n",
    "def source(files,n):\n",
    "    data,header=fits.getdata(files[n] ,header=True)\n",
    "    sigma_clip = SigmaClip(sigma=3, maxiters=10)\n",
    "    bkg_estimator = SExtractorBackground()\n",
    "    bkg = Background2D(data, (10,10), filter_size=(3, 3),sigma_clip=sigma_clip, bkg_estimator=bkg_estimator)\n",
    "    back=bkg.background\n",
    "    h = fits.open(filename)\n",
    "    wcs = WCS(h[0].header)\n",
    "    mask = data == 0\n",
    "    unit = u.electron / u.s\n",
    "\n",
    "    xdf_image = CCDData(data, unit=unit, meta=header, mask=mask)\n",
    "    norm_image = ImageNormalize(vmin=1e-4, vmax=5e-2, stretch=LogStretch(), clip=False)\n",
    "    xdf_image_clipped = np.clip(xdf_image, 1e-4, None)\n",
    "\n",
    "    mean, median, std = sigma_clipped_stats(xdf_image.data, sigma=3.0, maxiters=20, mask=xdf_image.mask)\n",
    "    daofind = DAOStarFinder(fwhm=4, threshold=5*std)\n",
    "    return daofind(data - back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de309492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting magnitude and magnitude error:\n",
    "#apertures is the variable that creates aperture of given radius and shape.\n",
    "#an_ap is the variable that creates annulus of given inner and outer radii and its shape.\n",
    "def magnitude(files, n,apertures, an_ap ):\n",
    "    data,header=fits.getdata(files[n] ,header=True)\n",
    "    sigma_clip = SigmaClip(sigma=3, maxiters=10)\n",
    "    bkg_estimator = SExtractorBackground()    \n",
    "    bkg = Background2D(data, (10,10), filter_size=(3, 3),sigma_clip=sigma_clip, bkg_estimator=bkg_estimator)\n",
    "    back=bkg.background\n",
    "    exposure=header['EXPTIME']\n",
    "    effective_gain=exposure\n",
    "    error=calc_total_error(data,back,effective_gain)\n",
    "\n",
    "    phot_table = aperture_photometry(data-back, apertures,error=error)\n",
    "    phot_table2=aperture_photometry(data-back,an_ap)\n",
    "\n",
    "    bkg_mean = phot_table2['aperture_sum'] / an_ap.area\n",
    "    bkg_sum = bkg_mean * an_ap.area\n",
    "\n",
    "    final_sum0=phot_table['aperture_sum_0']-bkg_sum\n",
    "    final_sum1=phot_table['aperture_sum_1']-bkg_sum\n",
    "    final_sum2=phot_table['aperture_sum_2']-bkg_sum\n",
    "    final_sum3=phot_table['aperture_sum_3']-bkg_sum\n",
    "    final_sum4=phot_table['aperture_sum_4']-bkg_sum\n",
    "            \n",
    "    mag_back=-2.5*np.log10(bkg_mean/exposure)\n",
    "    mag_0=-2.5*np.log10(final_sum0/exposure)\n",
    "    mag_1=-2.5*np.log10(final_sum1/exposure)\n",
    "    mag_2=-2.5*np.log10(final_sum2/exposure)\n",
    "    mag_3=-2.5*np.log10(final_sum3/exposure)\n",
    "    mag_4=-2.5*np.log10(final_sum4/exposure)\n",
    "\n",
    "    flux_err_0=phot_table['aperture_sum_err_0']\n",
    "    mag_err_0=1.09*flux_err_0/final_sum0\n",
    "\n",
    "    flux_err_1=phot_table['aperture_sum_err_1']\n",
    "    mag_err_1=1.09*flux_err_1/final_sum1\n",
    "\n",
    "    flux_err_2=phot_table['aperture_sum_err_2']\n",
    "    mag_err_2=1.09*flux_err_2/final_sum2\n",
    "\n",
    "    flux_err_3=phot_table['aperture_sum_err_3']\n",
    "    mag_err_3=1.09*flux_err_3/final_sum3\n",
    "\n",
    "    flux_err_4=phot_table['aperture_sum_err_4']\n",
    "    mag_err_4=1.09*flux_err_4/final_sum4 \n",
    "    return mag_0,mag_err_0,mag_1,mag_err_1,mag_2,mag_err_2 ,mag_3,mag_err_3,mag_4,mag_err_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c5fda",
   "metadata": {},
   "source": [
    "###### **B. Plotting aperture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02fe030",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=sorted(glob(os.path.join('/home/aries/projetFolder/NGC6709/NGC6709_1/20211009/S*cleaned.fits')))\n",
    "\n",
    "f=0\n",
    "filename= files[f]\n",
    "sources = source(files, f)\n",
    "\n",
    "positions = np.transpose((sources['xcentroid'], sources['ycentroid']))\n",
    "apertures = CircularAperture(positions, r=7)\n",
    "annulus_aperture = CircularAnnulus(positions, r_in=14, r_out=15)\n",
    "norm = simple_norm(data, 'linear', 'cool', percent=99)\n",
    "\n",
    "\n",
    "plt.xlim(300, 600)\n",
    "plt.ylim(1000, 1200)\n",
    "\n",
    "plt.imshow(data, norm=norm, interpolation='nearest')\n",
    "apertures.plot(color='blue', lw=1, alpha=0.5)\n",
    "ap_patches = apertures.plot(color='white', lw=0.5)\n",
    "ann_patches = annulus_aperture.plot(color='green', lw=0.05)\n",
    "handles = (ap_patches[0], ann_patches[0])\n",
    "plt.xlabel('X coordinate of frame')\n",
    "plt.ylabel('Y coordinate of frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef5e92",
   "metadata": {},
   "source": [
    "###### B. Optimum aperture:\n",
    "Growth curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42293c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CREATING MAGNITUDE APERTURE FILE FOR single FRAME\n",
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files=sorted(glob(os.path.join(tmp_dir, \"S-*cleaned.fits\")))\n",
    "\n",
    "f = 0\n",
    "source_0 = source(files , f)\n",
    "print('No. of sources: ',len(source_0))\n",
    "print(source_0)\n",
    "\n",
    "radii = [5, 6, 7, 8, 9]\n",
    "positions = [(source_0['xcentroid'][i], source_0['ycentroid'][i]) for i in range(len(source_0))]\n",
    "apertures = [CircularAperture(positions, r=r) for r in radii]\n",
    "an_ap = CircularAnnulus(positions, r_in=14, r_out=15)\n",
    "\n",
    "mag_0, mag_err_0, mag_1, mag_err_1, mag_2, mag_err_2, mag_3, mag_err_3, mag_4, mag_err_4 = magnitude(files, f,apertures, an_ap)\n",
    "jd = header_0['JD']\n",
    "jd_list = [jd] * len(source_0['xcentroid'])\n",
    "h = fits.open(files[f])\n",
    "wcs = WCS(h[0].header)\n",
    "ra , dec = wcs.all_pix2world(source_0['xcentroid'],source_0['ycentroid'],0)\n",
    "print(ra, dec)\n",
    "\n",
    "with open(f\"{tmp_dir}zipped_data_{f}.csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['JD', 'RA', 'DEC', 'Magnitude0', 'Magnitude_error0', 'Magnitude1', 'Magnitude_error1', 'Magnitude2', 'Magnitude_error2', 'Magnitude3', 'Magnitude_error3', 'Magnitude4', 'Magnitude_error4'])\n",
    "    writer.writerows(zip(jd_list, ra,dec, mag_0, mag_err_0, mag_1, mag_err_1, mag_2, mag_err_2, mag_3, mag_err_3, mag_4, mag_err_4))\n",
    "\n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTING \"growth curve\" TO CHOSE OPTIMUM APERTURE SIZE\n",
    "x=radii\n",
    "start = time.time()\n",
    "plt.figure(figsize = (8,8))\n",
    "for i in range(20,40):\n",
    "    y=[]\n",
    "    y.append(mag_4[i]-mag_0[i])\n",
    "    y.append(mag_4[i]-mag_1[i])\n",
    "    y.append(mag_4[i]-mag_2[i])\n",
    "    y.append(mag_4[i]-mag_3[i])\n",
    "    y.append(mag_4[i]-mag_4[i])  \n",
    "    plt.plot(x,y, 'ro-',markersize=2, lw=0.2, label='%i'%i)\n",
    "    #plt.ylim(-0.5,1)\n",
    "    plt.xlabel('Aperture diameter (pixels)', fontsize=15)\n",
    "    plt.ylabel('Largest aperture mag - aperture mag', fontsize = 15)\n",
    "    plt.xticks(fontsize = 15)\n",
    "    plt.yticks(fontsize = 15)\n",
    "#plt.legend()\n",
    "plt.title('Growth curve for 5 radii')\n",
    "plt.show()\n",
    "    \n",
    "end = time.time()\n",
    "print('Execution took: ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86741f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CREATING MAGNITUDE APERTURE FILE FOR ALL FRAMES\n",
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files=sorted(glob(os.path.join(tmp_dir, \"S-*cleaned.fits\")))\n",
    "\n",
    "radii = [5,6,7,8,9]\n",
    "for f in range(len(files)):\n",
    "    filename=files[f]\n",
    "    \n",
    "    print(files[f])\n",
    "    source_0 = source(files , f)\n",
    "    print('No. of sources:',len(source_0))\n",
    "\n",
    "    positions = [(source_0['xcentroid'][i], source_0['ycentroid'][i]) for i in range(len(source_0))]\n",
    "    apertures = [CircularAperture(positions, r=r) for r in radii]\n",
    "    an_ap = CircularAnnulus(positions, r_in=14, r_out=15)\n",
    "\n",
    "    mag_0, mag_err_0, mag_1, mag_err_1, mag_2, mag_err_2, mag_3, mag_err_3, mag_4, mag_err_4 = magnitude(files,f, apertures, an_ap)\n",
    "    jd = header_0['JD']\n",
    "    jd_list = [jd] * len(source_0['xcentroid'])\n",
    "    h = fits.open(files[f])\n",
    "    wcs = WCS(h[0].header)\n",
    "    ra , dec = wcs.all_pix2world(source_0['xcentroid'],source_0['ycentroid'],0)\n",
    "    \n",
    "    with open(f\"{tmp_dir}zipped_data/zipped_data_{f}.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['JD' ,'RA', 'DEC', 'Magnitude0', 'Magnitude_error0', 'Magnitude1', 'Magnitude_error1', 'Magnitude2', 'Magnitude_error2', 'Magnitude3', 'Magnitude_error3', 'Magnitude4', 'Magnitude_error4'])\n",
    "        writer.writerows(zip(jd_list, ra,dec, mag_0, mag_err_0, mag_1, mag_err_1, mag_2, mag_err_2, mag_3, mag_err_3, mag_4, mag_err_4))\n",
    "    \n",
    "    print('file',f+1,'generated')\n",
    "end = time.time()\n",
    "print('Execution took: ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d037c07",
   "metadata": {},
   "source": [
    "###### C. Calibration of Magnitude from simbad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a4ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SIMBAD CODE\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read RA and DEC from the text file\n",
    "star_coordinates = []\n",
    "with open(f\"{tmp_dir}zipped_data/zipped_data_0.csv\", 'r') as file: #\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        ra, dec = row[1], row[2]\n",
    "        star_coordinates.append((float(ra), float(dec)))\n",
    "\n",
    "# Query Simbad for magnitudes of stars\n",
    "Simbad.reset_votable_fields()\n",
    "Simbad.add_votable_fields('flux(R)', 'flux_error(R)')\n",
    "\n",
    "ra_list = []\n",
    "dec_list = []\n",
    "magnitude_list = []\n",
    "magnitude_error_list = []\n",
    "print('reached 1')\n",
    "for ra, dec in star_coordinates:\n",
    "    coords = SkyCoord(ra=ra, dec=dec, unit=u.deg, frame='icrs')\n",
    "    result_table = Simbad.query_region(coords, radius=5 * u.arcsec)\n",
    "    if result_table is not None:\n",
    "        magnitude = result_table['FLUX_R'][0]\n",
    "        magnitude_error = result_table['FLUX_ERROR_R'][0]\n",
    "        ra_list.append(ra)\n",
    "        dec_list.append(dec)\n",
    "        magnitude_list.append(magnitude)\n",
    "        magnitude_error_list.append(magnitude_error)\n",
    "    else:\n",
    "        ra_list.append(ra)\n",
    "        dec_list.append(dec)\n",
    "        magnitude_list.append('Notfound')\n",
    "        magnitude_error_list.append('Notfound')\n",
    "        \n",
    "    print('done')\n",
    "print('reached 2')\n",
    "\n",
    "# Save the retrieved magnitudes to a .csv file\n",
    "output_file = f\"{tmp_dir}simbad_magnitudes.csv\"\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['RA', 'DEC', 'Magnitude', 'Magnitude_error'])\n",
    "    writer.writerows(zip(ra_list, dec_list, magnitude_list, magnitude_error_list))\n",
    "\n",
    "print(f\"Star magnitudes saved to '{output_file}'.\")\n",
    "end = time.time()\n",
    "print('Execution took: ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27eb5b4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    " NOTE: RUNNING SIMBAD CODE IS LUCK, SOMETIMES IT DOESN'T WORK AND MANYTIMES IT WORKS BUT BELOW CODE DON'T GIVE LINEAR PLOT! BE PATIENT...! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3df47e",
   "metadata": {},
   "source": [
    "$y = mx+c$ equation of best fit line. Histogram of intercept $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read the CSV files\n",
    "file1 = pd.read_csv(f\"{tmp_dir}zipped_data/zipped_data_0.csv\") #reference frame file\n",
    "file2 = pd.read_csv(f\"{tmp_dir}simbad_magnitudes.csv\") #simbad code generated file\n",
    "\n",
    "# Extract the desired columns\n",
    "y_values = file2['Magnitude']  \n",
    "y_error = file2['Magnitude_error'] \n",
    "\n",
    "x_values2 = file1['Magnitude2']  \n",
    "x_error2 = file1['Magnitude_error2']\n",
    "\n",
    "# Convert y_values and y_error to numeric, handling invalid or missing values\n",
    "y_values = pd.to_numeric(y_values, errors='coerce')\n",
    "y_error = pd.to_numeric(y_error, errors='coerce')\n",
    "\n",
    "# Filter out invalid or missing values from x_values, y_values, and y_error\n",
    "valid_indices = (~np.isnan(x_values0)) & (~np.isnan(y_values)) & (~np.isnan(y_error))\n",
    "\n",
    "x_values2 = x_values2[valid_indices]\n",
    "x_error2 = x_error2[valid_indices]\n",
    "y_values = y_values[valid_indices]\n",
    "y_error = y_error[valid_indices]\n",
    "\n",
    "print('Number of sources detected by simabd:', len(x_values2)) #NUMBER OF SOURCES DETECTED BY SIMBAD\n",
    "\n",
    "# Plotting with errorbar\n",
    "plt.errorbar(x_values2, y_values,xerr=x_error2, yerr=y_error, color='blue',fmt='.', label='Data')\n",
    "\n",
    "slope2, intercept2 = np.polyfit(x_values2, y_values, 1)\n",
    "best_fit_line2 = slope2 * x_values2 + intercept2\n",
    "# Plot the best-fit line\n",
    "plt.plot(x_values2, best_fit_line2, color='blue', label='Best Fit Line 2')\n",
    "equation2 = f'y = {slope2:.2f}x + {intercept2:.2f}'\n",
    "\n",
    "plt.text(0.5, 0.85, equation2,color = 'blue', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "plt.xlabel('Magnitude from code') \n",
    "plt.ylabel('Magnitude from simbad')  \n",
    "plt.title('Calibration of magnitude') \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mag=y_values - x_values2\n",
    "plt.hist(mag, bins=25)\n",
    "plt.xlabel('Calibration Magnitude')\n",
    "plt.title('Histogram to correctly pick calibration magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = pd.read_csv(f'{tmp_dir}zipped_data/zipped_data_0.csv')  # reference frame file\n",
    "file2 = pd.read_csv(f'{tmp_dir}simbad_magnitudes.csv')  # simbad code generated file\n",
    "\n",
    "x_values2 = file1['Magnitude2']  \n",
    "y_values = file2['Magnitude'] \n",
    "\n",
    "y_values = pd.to_numeric(y_values, errors='coerce')\n",
    "\n",
    "valid_indices = (~np.isnan(x_values2)) & (~np.isnan(y_values))\n",
    "x_values2 = x_values2[valid_indices]\n",
    "y_values = y_values[valid_indices]\n",
    "\n",
    "slope2, intercept2 = np.polyfit(x_values2, y_values, 1)\n",
    "best_fit_line2 = slope2 * x_values2 + intercept2\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "counts, x_bins, y_bins, im = ax.hist2d(x_values2, y_values, bins=25,cmap='Reds')\n",
    "\n",
    "ax.plot(x_values2, best_fit_line2, color='red', label='Best Fit Line')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel('Density')\n",
    "\n",
    "ax.set_xlabel('DFOT Magnitude Values')\n",
    "ax.set_ylabel('SIMBAD Magnitude Values')\n",
    "ax.set_title('Calibration of magnitude (Contour Plot)')\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3de13",
   "metadata": {},
   "source": [
    "###### D. Redefining and Calibrating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a23c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefined_magnitude()\n",
    "#This is newly defined magnitude fuunction which \n",
    "\n",
    "def redefined_magnitude(files,n, apertures, an_ap ):\n",
    "    data,header=fits.getdata(files[n] ,header=True)\n",
    "    sigma_clip = SigmaClip(sigma=3, maxiters=10)\n",
    "    bkg_estimator = SExtractorBackground()    \n",
    "    bkg = Background2D(data, (10,10), filter_size=(3, 3),sigma_clip=sigma_clip, bkg_estimator=bkg_estimator)\n",
    "    back=bkg.background\n",
    "    exposure=header['EXPTIME']\n",
    "    effective_gain=exposure\n",
    "    error=calc_total_error(data,back,effective_gain)\n",
    "\n",
    "    phot_table = aperture_photometry(data-back, apertures,error=error)\n",
    "    phot_table2=aperture_photometry(data-back,an_ap)\n",
    "\n",
    "    bkg_mean = phot_table2['aperture_sum'] / an_ap.area\n",
    "    bkg_sum = bkg_mean * an_ap.area\n",
    "\n",
    "    final_sum0=phot_table['aperture_sum_0']-bkg_sum         \n",
    "    mag_back=-2.5*np.log10(bkg_mean/exposure)\n",
    "    mag_0=-2.5*np.log10(final_sum0/exposure)\n",
    "    flux_err_0=phot_table['aperture_sum_err_0']\n",
    "    mag_err_0=1.09*flux_err_0/final_sum0\n",
    "    return mag_0 , mag_err_0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67098ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibrating reference frame (single)\n",
    "#change def of magnitude before running this code!!\n",
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files=sorted(glob(os.path.join(tmp_dir, \"S-*cleaned.fits\")))\n",
    "\n",
    "f = 0\n",
    "print(files[f])\n",
    "source_0 = source(files,f)\n",
    "print('No. of sources: ',len(source_0))\n",
    "#-----------------------------------------------------------------\n",
    "intercept= 22.11 #needs to be changed as per above plot\n",
    "slope =1 #change only if error is more than 10-15%\n",
    "\n",
    "radius = 7 #chose optimum aperture size\n",
    "positions = [(source_0['xcentroid'][i], source_0['ycentroid'][i]) for i in range(len(source_0))]\n",
    "apertures = CircularAperture(positions, r=radius)\n",
    "an_ap = CircularAnnulus(positions, r_in=14, r_out=15)\n",
    "\n",
    "mag_0, mag_err_0 = redefined_magnitude(files,f,apertures, an_ap)\n",
    "\n",
    "calibrated_mag = slope*mag_0 + intercept #calibration step\n",
    "\n",
    "jd = header_0['JD']\n",
    "jd_list = [jd] * len(source_0['xcentroid'])\n",
    "h = fits.open(files[f])\n",
    "wcs = WCS(h[0].header)\n",
    "ra , dec = wcs.all_pix2world(source_0['xcentroid'],source_0['ycentroid'],0)\n",
    "print(ra, dec)\n",
    "data = np.array([jd_list, ra, dec, mag_0, mag_err_0])\n",
    "transposed_data = np.transpose(data)\n",
    "\n",
    "with open(f\"{tmp_dir}Calibrated_zipped_data_{f}.csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['JD', 'RA', 'DEC', 'Calibrated_Magnitude', 'Magnitude_error0'])\n",
    "    writer.writerows(zip(jd_list, ra,dec, calibrated_mag, mag_err_0))\n",
    "\n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94077dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTING HISTOGRAM ALONG WITH BRIGHT SOURCE BOUNDARY\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(f\"{tmp_dir}Calibrated_zipped_data_{f}.csv\")\n",
    "\n",
    "column_3_data = data['Calibrated_Magnitude']  \n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(column_3_data, bins=100)  # You can adjust the number of bins as per your preference\n",
    "plt.axvline(x=17, color='r', linestyle='--', label='x=17')\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Values')  \n",
    "plt.ylabel('Frequency')  \n",
    "plt.title('Histogram of Calibrated magnitude (ref) Data')  #\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ab6b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CREATING MAGNITUDE APERTURE FILE FOR ALL FRAMES\n",
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "files = sorted(glob(os.path.join(tmp_dir, \"S-*cleaned.fits\")))\n",
    "\n",
    "radius = 7      #chose optimum aperture size\n",
    "intercept= 22.11 #needs to be changed as per above plot\n",
    "slope = 1\n",
    "for f in range(len(files)):\n",
    "\n",
    "    print(files[f])\n",
    "    source_0 = source(files , f)\n",
    "    print('No. of sources: ',len(source_0))\n",
    "\n",
    "    positions = [(source_0['xcentroid'][i], source_0['ycentroid'][i]) for i in range(len(source_0))]\n",
    "    apertures = CircularAperture(positions, r=radius)\n",
    "    an_ap = CircularAnnulus(positions, r_in=14, r_out=15)\n",
    "\n",
    "    mag_0, mag_err_0 = redefined_magnitude(files, f,apertures, an_ap)\n",
    "    \n",
    "    calibrated_mag = slope*mag_0 + intercept\n",
    "    \n",
    "    jd = header_0['JD']\n",
    "    jd_list = [jd]*len(source_0['xcentroid'])\n",
    "    h = fits.open(files[f])\n",
    "    wcs = WCS(h[0].header)\n",
    "    ra , dec = wcs.all_pix2world(source_0['xcentroid'],source_0['ycentroid'],0)\n",
    "\n",
    "    with open(f\"{tmp_dir}calibrated_zipped_data/Calibrated_zipped_data_{f}.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['JD' ,'RA', 'DEC', 'Calibrated_Magnitude', 'Magnitude_error0'])\n",
    "        writer.writerows(zip(jd_list, ra, dec, calibrated_mag, mag_err_0))\n",
    "    \n",
    "    print('file created for', f+1)   \n",
    "end = time.time()\n",
    "print('Execution took: ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c522316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting calibrated magnitude files  wrt calibrated magnitude\n",
    "\n",
    "f=0\n",
    "csv_file = f\"{tmp_dir}calibrated_zipped_data/Calibrated_zipped_data_{f}.csv\"\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Sort the data based on column 4\n",
    "sorted_data = data.sort_values(by='Calibrated_Magnitude')\n",
    "\n",
    "# Save the sorted data to a new CSV file\n",
    "sorted_data.to_csv( f\"{tmp_dir}Calibrated_zipped_data_{f}.csv\", index=False) \n",
    "\n",
    "print(\"CSV file sorted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f9764",
   "metadata": {},
   "source": [
    "### **Plotting Light Curves**\n",
    "method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c71459",
   "metadata": {},
   "source": [
    "###### A. Ploting light curves for individual star\n",
    "more is the magnitude, more is the associated error!\n",
    "\n",
    "check if this trend is followed below, as we have sorted the calibrated magnitude file above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a20e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ploting light curve for single star\n",
    "\n",
    "start =time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files = sorted(glob(os.path.join(tmp_dir, \"S-*cleaned.fits\")))\n",
    "# Read the reference file \n",
    "reference_file = pd.read_csv(f\"{tmp_dir}Calibrated_zipped_data_0.csv\")\n",
    "\n",
    "sn = 11 #star number\n",
    "\n",
    "reference_ra = reference_file['RA'][sn] \n",
    "reference_dec = reference_file['DEC'][sn]\n",
    "\n",
    "matching_jd = []\n",
    "matching_magnitude = []\n",
    "matching_mag_error = []\n",
    "for i in range(1, len(files)):  \n",
    "    target_file = pd.read_csv(f\"{tmp_dir}calibrated_zipped_data/Calibrated_zipped_data_{i}.csv\")\n",
    "    \n",
    "    # Check if any RA and DEC values in the target file match with the reference values within the tolerance\n",
    "    matches = ((target_file['RA'] - reference_ra).abs() <= 0.001) & ((target_file['DEC'] - reference_dec).abs() <= 0.001)\n",
    "    \n",
    "    # If there is a match, append the JD and magnitude values to the respective lists\n",
    "    if matches.any():\n",
    "        matching_jd.append(target_file.loc[matches, 'JD'].iloc[0])\n",
    "        matching_magnitude.append(target_file.loc[matches, 'Calibrated_Magnitude'].iloc[0])\n",
    "        matching_mag_error.append(target_file.loc[matches, 'Magnitude_error0'].iloc[0])\n",
    "print('Number of data points:', len(matching_jd), len(matching_magnitude),len(matching_mag_error))\n",
    "print(matches)\n",
    "\n",
    "# Plot the magnitudes of the matching target files against their JD values\n",
    "#plt.plot(matching_jd, matching_magnitude,'o')\n",
    "plt.errorbar(matching_jd, matching_magnitude, yerr=matching_mag_error, color='blue',fmt='.', label='Data')\n",
    "plt.xlabel('JD')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Light curve for %s star' %sn)\n",
    "plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print('Execution took: ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecec77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ploting  light cuves for desired number of stars\n",
    "# Read the reference file\n",
    "reference_file = pd.read_csv(f\"{tmp_dir}Calibrated_zipped_data_0.csv\")\n",
    "start = time.time()\n",
    "start_index = 8 # Index of the first row to consider\n",
    "end_index = 90 # Index of the last row to consider\n",
    "# Iterate over the RA and DEC values in the reference file\n",
    "for index, row in reference_file.iloc[start_index:end_index].iterrows():\n",
    "    reference_ra = row['RA']\n",
    "    reference_dec = row['DEC']\n",
    "\n",
    "    # Initialize lists to store the matching target files' JD and magnitude values\n",
    "    matching_jd = []\n",
    "    matching_magnitude = []\n",
    "    matching_mag_error = []\n",
    "    \n",
    "    for i in range(1, len(files)):  \n",
    "        target_file = pd.read_csv(f\"{tmp_dir}calibrated_zipped_data/Calibrated_zipped_data_{i}.csv\")\n",
    "\n",
    "        # Check if any RA and DEC values in the target file match with the reference values within the tolerance\n",
    "        matches = (\n",
    "            (target_file['RA'] - reference_ra).abs() <= 0.001\n",
    "        ) & (\n",
    "            (target_file['DEC'] - reference_dec).abs() <= 0.001\n",
    "        )\n",
    "\n",
    "        # If there is a match, append the JD and magnitude values to the respective lists\n",
    "        if matches.any():\n",
    "            matching_jd.append(target_file.loc[matches, 'JD'].iloc[0])\n",
    "            matching_magnitude.append(target_file.loc[matches, 'Calibrated_Magnitude'].iloc[0])\n",
    "            matching_mag_error.append(target_file.loc[matches, 'Magnitude_error0'].iloc[0])\n",
    "    print('Number of data points:',len(matching_jd), len(matching_magnitude))\n",
    "    print(index)\n",
    "    \n",
    "    # Plot the magnitudes of the matching target files against their JD values\n",
    "    plt.errorbar(matching_jd, matching_magnitude, yerr=matching_mag_error, color='blue',markersize=1,fmt='.', label='Data')\n",
    "    plt.xlabel('JD')\n",
    "    plt.ylim(8.4,9.5)\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.title(f'Magnitudes of star with RA={reference_ra} and DEC={reference_dec}')\n",
    "    plt.show()\n",
    "end = time.time()\n",
    "print('Execution took: ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8ef32",
   "metadata": {},
   "source": [
    "###### B. Plotting Magnitude error vs Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = tmp_dir + 'Calibrated_zipped_data_0.csv'\n",
    "\n",
    "Magnitude_error_col = []\n",
    "Calibrated_Magnitude_col = []\n",
    "\n",
    "# Read the CSV file\n",
    "with open(csv_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    next(csv_reader)  \n",
    "    \n",
    "    for row in csv_reader:\n",
    "        if row[3] != '':\n",
    "            Magnitude_error_col.append(float(row[4]))  \n",
    "            Calibrated_Magnitude_col.append(float(row[3]))  \n",
    "            \n",
    "# Plotting the data\n",
    "plt.plot(Calibrated_Magnitude_col, Magnitude_error_col,'.', markersize=0.8)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--',label = 'y=0.5')\n",
    "plt.xlabel('Calibrated Magnitude')\n",
    "plt.ylim(-0.2,10)\n",
    "plt.ylabel('Magnitude error')\n",
    "plt.title('Magnitude error vs Magnitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...with more details\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('/home/aries/projetFolder/NGC6709/NGC6709_1/20211009/calibrated_zipped_data/Calibrated_zipped_data_0.csv')\n",
    "\n",
    "calibrated_magnitude = data['Calibrated_Magnitude']\n",
    "magnitude_error = data['Magnitude_error0']\n",
    "\n",
    "mask = ~np.isnan(calibrated_magnitude)\n",
    "\n",
    "calibrated_magnitude = calibrated_magnitude[mask]\n",
    "magnitude_error = magnitude_error[mask]\n",
    "\n",
    "# Define the number of bins for the histogram\n",
    "num_bins = 1000\n",
    "hist_bins = 100\n",
    "\n",
    "hist, x_edges, y_edges = np.histogram2d(calibrated_magnitude, magnitude_error, bins=num_bins)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1,ax2) = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "ax2.set_facecolor('white')\n",
    "\n",
    "ax1.hist(calibrated_magnitude, bins=hist_bins, color='lightblue', edgecolor='blue')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.tick_params(direction='in')\n",
    "ax1.set_title('Histogram and contour', fontweight='bold')\n",
    "\n",
    "# Create the contour plot with blue contours\n",
    "contour = ax2.contourf(x_edges[:-1], y_edges[:-1], hist.T, levels=100, cmap='Blues')\n",
    "ax2.set_ylim(-0, 8)\n",
    "ax2.tick_params(direction='in')\n",
    "plt.axvline(x=17, color='r', linestyle='--', label='x=17')\n",
    "plt.legend()\n",
    "\n",
    "ax2.set_xlabel('Calibrated Magnitude')\n",
    "ax2.set_ylabel('Magnitude Error')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.06)\n",
    "cbar = fig.add_axes([0.91, 0.15, 0.05, 0.3])\n",
    "plt.colorbar(contour, cax=cbar, orientation='vertical').set_label(label='Density',size=10,weight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff78db6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#and more...\n",
    "tmp_dir = '/home/aries/projetFolder/NGC6709/NGC6709_1/20211009/'\n",
    "df = pd.read_csv(f'{tmp_dir}calibrated_zipped_data/Calibrated_zipped_data_0.csv')\n",
    "\n",
    "# Remove rows with NaN values in the Calibrated_Magnitude column\n",
    "df = df.dropna(subset=['Calibrated_Magnitude'])\n",
    "\n",
    "# Plot density contour using seaborn\n",
    "ax=sns.kdeplot(data=df, x='Calibrated_Magnitude', y='Magnitude_error0', fill=True, cmap='Blues',n_levels=25)\n",
    "ax.text(11, 8, \"Night 1\",fontsize = 15,color = \"Black\",ha = \"center\", va = \"center\")\n",
    "\n",
    "plt.xlabel('Calibrated Magnitude')\n",
    "plt.ylabel('Magnitude Error')\n",
    "plt.ylim(-2,10)\n",
    "plt.title('Density Contour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27151095",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### **Plotting light curves from transposed data:** \n",
    "method 2 *for convenience and ease only*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e68ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Transpose of data\n",
    "start = time.time()\n",
    "\n",
    "directory_path = tmp_dir + \"calibrated_zipped_data/\"\n",
    "\n",
    "# Read the reference file - file with max number of stars detected\n",
    "reference_file = pd.read_csv(\"/home/aries/projetFolder/NGC6709/NGC6709_3/20211106/calibrated_zipped_data/Calibrated_zipped_data_234.csv\")\n",
    "target_Files = [(directory_path + f) for f in os.listdir(directory_path) if f.startswith('Calibrated_zipped_data')]\n",
    "target_Files.sort(key=lambda x: int(x.split('_')[6].split('.')[0]))\n",
    "\n",
    "# Filter reference file by 'Calibrated_magnitude' < 17\n",
    "reference_file = reference_file[reference_file['Calibrated_Magnitude'] < 17]\n",
    "\n",
    "# Iterate through each row in the reference file\n",
    "start_index = 0  # Index of the first row to consider\n",
    "end_index =  6500 # Index of the last row to consider\n",
    "\n",
    "metadata = pd.DataFrame(columns=['File', 'RA', 'DEC'])\n",
    "\n",
    "for index, row in reference_file.iloc[start_index:end_index].iterrows():\n",
    "    reference_ra = row['RA']\n",
    "    reference_dec = row['DEC']\n",
    "    \n",
    "    target_file_index = (index - start_index) % len(target_Files)\n",
    "    target_file = target_Files[target_file_index]\n",
    "\n",
    "    # Read the target file to obtain RA and DEC values\n",
    "    target_data = pd.read_csv(target_file)\n",
    "    target_ra = target_data['RA'].iloc[0]\n",
    "    target_dec = target_data['DEC'].iloc[0]\n",
    "    \n",
    "    # Create an empty DataFrame to store the results for each matching file\n",
    "    matching_results = pd.DataFrame(columns=['JD', 'Calibrated_Magnitude', 'Magnitude_error0'])\n",
    "\n",
    "    # Add the reference row to the matching_results DataFrame\n",
    "    matching_results.loc[len(matching_results)] = row[['JD', 'Calibrated_Magnitude', 'Magnitude_error0']]\n",
    "    unique_jd_values = set()\n",
    "\n",
    "    unique_jd_values.add(row['JD'])\n",
    "\n",
    "    # Iterate through each file (excluding the reference file)\n",
    "    for i in range(0,len(target_Files)):\n",
    "        filename = f\"{tmp_dir}calibrated_zipped_data/Calibrated_zipped_data_{i}.csv\"  # Assuming the file names are in the format '1.csv', '2.csv', ...\n",
    "        file_data = pd.read_csv(filename)\n",
    "\n",
    "        matching_data = file_data[np.sqrt((np.square(file_data['RA'] - reference_ra)) + (np.square(file_data['DEC'] - reference_dec))) < 1.41/3600]\n",
    "\n",
    "        # Iterate through the matching data rows\n",
    "        for _, matching_row in matching_data.iterrows():\n",
    "            jd_value = matching_row['JD']\n",
    "\n",
    "            if jd_value not in unique_jd_values:\n",
    "                # Add a new row to the matching_results DataFrame\n",
    "                matching_results.loc[len(matching_results)] = matching_row[['JD', 'Calibrated_Magnitude', 'Magnitude_error0']]\n",
    "                # Add the JD value to the set of unique JD values\n",
    "                unique_jd_values.add(jd_value)\n",
    "\n",
    "    # Save the matching results to a CSV file\n",
    "    matching_results.to_csv(f\"{tmp_dir}transposed_data/Mag_file_star_{index}.csv\", index=False)\n",
    "    print('File created for:', index)\n",
    "    metadata.loc[len(metadata)] = [f\"{tmp_dir}transposed_data/Mag_file_star_{index}.csv\", reference_ra, reference_dec]\n",
    "\n",
    "    \n",
    "metadata_filename = f\"{tmp_dir}metadata.csv\"\n",
    "metadata.to_csv(metadata_filename, index=False)\n",
    "\n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTING LIGHT CURVE AND HISTOGRAM OF DEVIATION FROM MEAN using transposed data\n",
    "\n",
    "# Read the reference file\n",
    "reference_file = pd.read_csv(f\"{tmp_dir}Calibrated_zipped_data_0.csv\")\n",
    "directory_path = tmp_dir + \"transposed_data/\"\n",
    "csv_files = [(directory_path + f) for f in os.listdir(directory_path) if f.startswith('Mag_file_star')]\n",
    "csv_files.sort(key=lambda x: int(x.split('_')[5].split('.')[0]))\n",
    "\n",
    "df = pd.read_csv(csv_files[80]) #any file from files\n",
    "\n",
    "magni_tude = [df.iloc[:,1]]\n",
    "print(np.mean(magni_tude))\n",
    "print(magni_tude)\n",
    "diff1=[]\n",
    "for i in range(100):\n",
    "    diff =float(df.iloc[i, 1] - np.mean(magni_tude))\n",
    "    diff1.append(diff)\n",
    "        \n",
    "plt.plot(df.iloc[:, 0], df.iloc[:, 1], 'o')\n",
    "  # Adjust the width and height as needed\n",
    "plt.xlabel('JD')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Light curve')\n",
    "plt.axhline(y=np.mean(magni_tude), color='r', linestyle='--', label='mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(diff1)\n",
    "if diff1 != 'nan':\n",
    "    plt.hist(diff1, bins=50)\n",
    "    plt.xlabel('values')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title('Deviation from mean')\n",
    "#plt.axvline(x=np.mean(magni_tude), color='r', linestyle='--',label='mean')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b2d57",
   "metadata": {},
   "source": [
    "### **Differential light curves**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba760138",
   "metadata": {},
   "source": [
    "###### A. Minimum difference using transposed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_first_column(csv_files):\n",
    "    first_column_values = set()\n",
    "    first_column_lengths = set()\n",
    "    \n",
    "    for file in csv_files:\n",
    "        with open(file, 'r') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            first_column = []\n",
    "            \n",
    "            for row in reader:\n",
    "                first_column.append(row[0])\n",
    "            \n",
    "            first_column_values.add(tuple(first_column))\n",
    "            first_column_lengths.add(len(first_column))\n",
    "    \n",
    "    return len(first_column_values) == 1 and len(first_column_lengths) == 1\n",
    "\n",
    "# Directory path containing the CSV files\n",
    "directory_path = tmp_dir + \"transposed_data/\"\n",
    "\n",
    "csv_files = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.startswith('Mag_file_star') and filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        csv_files.append(file_path)\n",
    "\n",
    "# Check if all CSV files have the same first column\n",
    "if check_first_column(csv_files):\n",
    "    print(\"All CSV files have the same first column.\")\n",
    "else:\n",
    "    print(\"CSV files have different values or lengths in the first column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cef6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "tmp_dir = '/home/aries/projetFolder/NGC6709/NGC6709_1/20211009/'\n",
    "\n",
    "# Set the directory path where the CSV files are located\n",
    "directory_path = f'{tmp_dir}transposed_data/'\n",
    "# Set the chosen number for the range\n",
    "x = 14\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [(directory_path + f) for f in os.listdir(directory_path) if f.startswith('Mag_file_')]\n",
    "print(len(csv_files))\n",
    "# Create an empty list to store the filtered file names\n",
    "filtered_files = []\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for file in csv_files:\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    # Calculate the lower and upper bounds for the range\n",
    "    lower_bound = x - 1\n",
    "    upper_bound = x + 1\n",
    "    \n",
    "    # Filter the data based on the condition\n",
    "    filtered_data = data[(data['Calibrated_Magnitude'] >= lower_bound) & (data['Calibrated_Magnitude'] <= upper_bound)]\n",
    "    \n",
    "    # Calculate the percentage of rows within the range\n",
    "    percentage_within_range = len(filtered_data) / len(data) * 100\n",
    "\n",
    "    # Check if the percentage meets the condition of 90%\n",
    "    if percentage_within_range >= 90:\n",
    "        filtered_files.append(file)\n",
    "print(len(filtered_files))\n",
    "\n",
    "# Print the filtered file names\n",
    "filtered_files.sort(key=lambda x: int(x.split('_')[5].split('.')[0]))\n",
    "for file in filtered_files:\n",
    "    print(file)\n",
    "    \n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39741d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "folder_path = tmp_dir + 'transposed_data'  # Replace with the actual folder path\n",
    "\n",
    "# # Get a list of all CSV files in the folder\n",
    "# csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "csv_files= filtered_files\n",
    "csv_files.sort(key=lambda x: int(x.split('_')[5].split('.')[0]))\n",
    "#csv_files=csv_files[1:5]\n",
    "print(len(csv_files))\n",
    "# Generate all possible combinations of two CSV files\n",
    "# Generate all possible combinations of two CSV files\n",
    "file_combinations = list(combinations(csv_files, 2))\n",
    "\n",
    "# Initialize variables to track minimum standard deviation and corresponding file pair\n",
    "min_std_dev = float('inf')\n",
    "min_std_dev_files = None\n",
    "sigma_list=[]\n",
    "i=0\n",
    "# Read and process each file combination\n",
    "for file_pair in file_combinations:\n",
    "    reference_file = file_pair[0]\n",
    "    compare_file = file_pair[1]\n",
    "    \n",
    "    reference_file_path = os.path.join(folder_path, reference_file)\n",
    "    compare_file_path = os.path.join(folder_path, compare_file)\n",
    "    \n",
    "    reference_df = pd.read_csv(reference_file_path)\n",
    "    compare_df = pd.read_csv(compare_file_path)\n",
    "    \n",
    "    # Merge the reference and compare DataFrames based on the 'JD' column\n",
    "    merged_df = pd.merge(reference_df, compare_df, on='JD', how='inner')\n",
    "    \n",
    "    # Perform subtraction between corresponding 'Calibrated_Magnitude' values and store the result in a list\n",
    "    subtraction_list = merged_df['Calibrated_Magnitude_x'] - merged_df['Calibrated_Magnitude_y']\n",
    "    \n",
    "    # Calculate the standard deviation of the subtraction list\n",
    "    std_dev = subtraction_list.std()\n",
    "    \n",
    "    # Append the standard deviation to the 'sigma_list'\n",
    "    sigma_list.append(std_dev)\n",
    "    \n",
    "    # Check if the current standard deviation is the minimum\n",
    "    if std_dev < min_std_dev:\n",
    "        min_std_dev = std_dev\n",
    "        min_std_dev_files = (reference_file, compare_file)\n",
    "    \n",
    "    # Print the subtraction list and its standard deviation for the current file combination\n",
    "    print(f\"Subtraction list for {reference_file} and {compare_file}:\")\n",
    "    #print(subtraction_list)\n",
    "    print(f\"Standard Deviation: {std_dev}\")\n",
    "    print()\n",
    "    i +=1\n",
    "# Print the file pair corresponding to the minimum standard deviation\n",
    "print(\"File pair with minimum standard deviation:\")\n",
    "print(min_std_dev_files)\n",
    "print(\"Minimum Standard Deviation:\")\n",
    "print(min_std_dev)\n",
    "print('total combinations', i)\n",
    "\n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ffbb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the list in ascending order\n",
    "sorted_numbers = sorted(sigma_list)\n",
    "\n",
    "# Extract the top 10 least elements\n",
    "top_10_least = sorted_numbers[:30]\n",
    "\n",
    "# Print the top 10 least elements\n",
    "print(\"Top 10 least numbers:\")\n",
    "for number in top_10_least:\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836805d",
   "metadata": {},
   "source": [
    "###### B. **PLOTING DIFFERENTIAL LIGHT CURVES** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTING LIGHT CURVES OF STARS WITH MAG RANGE (+-1) of x\n",
    "\n",
    "# check for bin size\n",
    "start = time.time()\n",
    "\n",
    "# Read the first CSV file\n",
    "df1 = pd.read_csv(f'{tmp_dir}transposed_data/Mag_file_star_245.csv')\n",
    "# plt.plot(df1.iloc[:, 0],df1.iloc[:, 1],'.')\n",
    "# plt.show()\n",
    "\n",
    "df2 = pd.read_csv(f'{tmp_dir}transposed_data/Mag_file_star_270.csv')\n",
    "for i in filtered_files[:5]: #here enter only chosen bin \n",
    "    \n",
    "    df3 = pd.read_csv(i)\n",
    "    # Extract the first and second columns from both dataframes\n",
    "    x = df2.iloc[:, 0]  # 1st column of file1 (assuming index starts from 0) \n",
    "    if len(df1)==len(df2)==len(df3):\n",
    "        diff1 = df3.iloc[:, 1] - df1.iloc[:, 1]  \n",
    "        diff2 = df2.iloc[:, 1] - df3.iloc[:, 1]\n",
    "        diff3 = df1.iloc[:, 1] - df2.iloc[:, 1]\n",
    "               \n",
    "        #plt.axhline(y=np.mean(diff3), color='r', linestyle='--', label='mean of C1-C2')\n",
    "        plt.plot(x, diff1, '.', label='T-C1')\n",
    "        plt.plot(x, diff2,'.',label='T-C2')    \n",
    "        plt.plot(x, diff3 ,'.',label='C1-C2')\n",
    "        \n",
    "        # Add labels and title\n",
    "        plt.ylabel('Differential magnitude')\n",
    "        plt.xlabel('JD')\n",
    "        plt.title(f'Difference of Magnitude {i}')\n",
    "        plt.legend()\n",
    "        #plt.xlim(2459497.1037037, 2459497.17280815)\n",
    "        plt.show()\n",
    "    \n",
    "end = time.time()\n",
    "print('Execution took: ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "\n",
    "for file_name in matched_file_paths:\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data.sort_values('JD')\n",
    "    print(file_name)\n",
    "    print(type(data['JD']))\n",
    "    frame_list = data['JD'].tolist()\n",
    "    print(type(frame_list))\n",
    "    jd_lists = make_jd_list(file_name)\n",
    "    \n",
    "    bin_size = 30\n",
    "    num_bins = len(data) // bin_size\n",
    "    print(bin_size,num_bins)\n",
    "    binned_data = pd.DataFrame(columns=['JD', 'Calibrated_Magnitude', 'Magnitude_error0'])\n",
    "\n",
    "    for i in range(num_bins):\n",
    "    # Determine the start and end indices for each bin\n",
    "        start_index = i * bin_size\n",
    "        end_index = start_index + bin_size\n",
    "\n",
    "    # Extract the data points within the current bin\n",
    "        bin_points = data.iloc[start_index:end_index]\n",
    "        std_dev = np.std(np.array(jd_lists[i], dtype=float))\n",
    "    # Calculate the mean values for the bin\n",
    "        mean_jd = bin_points['JD'].mean()\n",
    "        mean_mag = bin_points['Calibrated_Magnitude'].mean()\n",
    "        mean_error = bin_points['Magnitude_error0'].mean()\n",
    "    \n",
    "    # Append the binned data to the DataFrame\n",
    "        bin_data = pd.DataFrame({'JD': [mean_jd],'JD_error': [std_dev], 'Calibrated_Magnitude': [mean_mag], 'Magnitude_error0': [mean_error]})\n",
    "        binned_data = pd.concat([binned_data, bin_data], ignore_index=True)\n",
    "\n",
    "    diff_binned_data =[]\n",
    "    for i in range(0,len(binned_data['JD'])):\n",
    "        a = binned_data['JD'][i] - binned_data['JD'][0]\n",
    "        diff_binned_data.append(a)\n",
    "    \n",
    "#     print(binned_data['JD_error'])\n",
    "# Plot the binned data with error bars\n",
    "    plt.errorbar(binned_data['JD'] , binned_data['Calibrated_Magnitude'],xerr=binned_data['JD_error'], yerr=binned_data['Magnitude_error0'],markersize=1, fmt='o')\n",
    "\n",
    "    plt.xlabel('JD')\n",
    "    plt.ylabel('Calibrated Magnitude')\n",
    "    plt.title('Plot of Calibrated Magnitude vs. JD')\n",
    "\n",
    "# Add a legend\n",
    "#plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "for r in matched_file_paths:\n",
    "    csv_file = r\n",
    "    print(csv_file)\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data = data.sort_values('JD')\n",
    "\n",
    "    jd_lists = make_jd_list(csv_file)\n",
    "    print(len(jd_lists[0]))\n",
    "\n",
    "    binned_data = pd.DataFrame(columns=['JD','JD_error', 'Calibrated_Magnitude', 'Magnitude_error0'])\n",
    "\n",
    "# Calculate the standard deviation\n",
    "    std_dev = np.std(np.array(jd_lists[0], dtype=float))\n",
    "    n = len(jd_lists[0])\n",
    "    std_error = std_dev / np.sqrt(n)\n",
    "    print(std_dev)\n",
    "    print(r)\n",
    "# Iterate over the bins\n",
    "    for i in range(num_bins):\n",
    "    # Determine the start and end indices for each bin\n",
    "        start_index = i * bin_size\n",
    "        end_index = start_index + bin_size\n",
    "\n",
    "    # Extract the data points within the current bin\n",
    "        bbin_points = data.iloc[start_index:end_index]\n",
    "        std_dev = np.std(np.array(jd_lists[i], dtype=float))\n",
    "    # Calculate the mean values for the bin\n",
    "        mean_jd = bin_points['JD'].mean()\n",
    "        mean_mag = bin_points['Calibrated_Magnitude'].mean()\n",
    "        mean_error = bin_points['Magnitude_error0'].mean()\n",
    "    \n",
    "        # Append the binned data to the DataFrame\n",
    "        bin_data = pd.DataFrame({'JD': [mean_jd],'JD_error': [std_dev], 'Calibrated_Magnitude': [mean_mag], 'Magnitude_error0': [mean_error]})\n",
    "        binned_data = pd.concat([binned_data, bin_data], ignore_index=True)\n",
    "    \n",
    "    print(binned_data['JD'])\n",
    "    diff_binned_data =[]\n",
    "    for i in range(len(binned_data['JD'])):\n",
    "        a = binned_data['JD'][i] - binned_data['JD'][0]\n",
    "        diff_binned_data.append(a)\n",
    "        \n",
    "    \n",
    "# Plot the binned data with error bars\n",
    "    plt.errorbar(diff_binned_data, binned_data['Calibrated_Magnitude'],xerr=binned_data['JD_error'], yerr=binned_data['Magnitude_error0'],markersize=5, fmt='o')\n",
    "#     for r in range(len(binned_data['Calibrated_Magnitude'])):\n",
    "#         if binned_data['Calibrated_Magnitude'][r] > 17:\n",
    "#             print(binned_data['Calibrated_Magnitude'][r])\n",
    "        \n",
    "    print(np.mean(binned_data['Calibrated_Magnitude']))\n",
    "    plt.axhline(y=np.mean(binned_data['Calibrated_Magnitude']),color='r')\n",
    "#plt.errorbar(binned_data['JD'], binned_data['Calibrated_Magnitude'],xerr=std_dev, yerr=binned_data['Magnitude_error0'],markersize=0.5, fmt='o')\n",
    "    plt.xlabel('JD in days')\n",
    "    plt.ylabel('Calibrated Magnitude')\n",
    "    plt.title('Binned Data of a star from 1st night')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ERROR BAR INCLUDED\n",
    "\n",
    "start = time.time()\n",
    "# Read the first CSV file\n",
    "df1 = pd.read_csv(f'{tmp_dir}transposed_data/Mag_file_star_396.csv')\n",
    "plt.plot(df1.iloc[:, 0], df1.iloc[:, 1], '.')\n",
    "\n",
    "# Read the second CSV file\n",
    "df2 = pd.read_csv(f'{tmp_dir}transposed_data/Mag_file_star_922.csv')\n",
    "\n",
    "for i in filtered_files:  # here enter only chosen bin\n",
    "    df3 = pd.read_csv(i)\n",
    "\n",
    "    # Extract the columns from all dataframes\n",
    "    x = df2.iloc[:, 0]  # 1st column of file2 (assuming index starts from 0)\n",
    "    \n",
    "    if len(df1) == len(df2) == len(df3):\n",
    "        diff1 = df3.iloc[:, 1] - df1.iloc[:, 1]\n",
    "        diff2 = df3.iloc[:, 1] - df2.iloc[:, 1]\n",
    "        diff3 = df1.iloc[:, 1] - df2.iloc[:, 1]\n",
    "        \n",
    "        error1 = df3.iloc[:, 2]  # Magnitude_error0 column of file3\n",
    "        error2 = df2.iloc[:, 2]  # Magnitude_error0 column of file2\n",
    "        error3 = df1.iloc[:, 2]  # Magnitude_error0 column of file1\n",
    "        \n",
    "        error_diff1 = np.sqrt(error1**2 + error2**2)\n",
    "        error_diff2 = np.sqrt(error2**2 + error3**2)\n",
    "        error_diff3 = np.sqrt(error3**2 + error1**2)\n",
    "        \n",
    "        plt.errorbar(x, diff1, yerr=error_diff1,markersize=2 ,fmt='.', label='T-C1')\n",
    "        plt.errorbar(x, diff2, yerr=error_diff2, markersize=2 ,fmt='.', label='T-C2')\n",
    "        plt.errorbar(x, diff3, yerr=error_diff3,markersize=1 , fmt='.', label='C1-C2')\n",
    "\n",
    "        plt.ylabel('Differential magnitude')\n",
    "        plt.xlabel('JD')\n",
    "        plt.title(f'Difference of Magnitude {i}')\n",
    "        plt.legend()\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "        \n",
    "end = time.time()\n",
    "print('Execution took: ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binnning and errorbar\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Read the first CSV file\n",
    "df1 = pd.read_csv(f'{tmp_dir}transposed_data/Mag_file_star_295.csv')\n",
    "\n",
    "# Read the second CSV file\n",
    "df2 = pd.read_csv(f'{tmp_dir}transposed_data/Mag_file_star_297.csv')\n",
    "\n",
    "# Define binning parameters\n",
    "bin_size = 25  # Number of data points per bin\n",
    "\n",
    "for i in filtered_files:  # here enter only chosen bin\n",
    "    df3 = pd.read_csv(i)\n",
    "\n",
    "    \n",
    "    x = df2.iloc[:, 0]  \n",
    "\n",
    "    if len(df1) == len(df2) == len(df3):\n",
    "        diff1 = df3.iloc[:, 1] - df1.iloc[:, 1]\n",
    "        diff2 = df3.iloc[:, 1] - df2.iloc[:, 1]\n",
    "        diff3 = df1.iloc[:, 1] - df2.iloc[:, 1]\n",
    "\n",
    "        \n",
    "        error1 = df3.iloc[:, 2]  \n",
    "        error2 = df2.iloc[:, 2]  \n",
    "        error3 = df1.iloc[:, 2]  \n",
    "\n",
    "        # Calculate the error in the difference using error propagation\n",
    "        error_diff1 = np.sqrt(error1**2 + error2**2)\n",
    "        error_diff2 = np.sqrt(error2**2 + error3**2)\n",
    "        error_diff3 = np.sqrt(error3**2 + error1**2)\n",
    "    \n",
    "        num_bins = len(x) // bin_size\n",
    "        x_binned = np.array_split(x, num_bins)\n",
    "\n",
    "        # Initialize arrays to store binned values and errors\n",
    "        diff1_binned = np.zeros(num_bins)\n",
    "        diff2_binned = np.zeros(num_bins)\n",
    "        diff3_binned = np.zeros(num_bins)\n",
    "        error_diff1_binned = np.zeros(num_bins)\n",
    "        error_diff2_binned = np.zeros(num_bins)\n",
    "        error_diff3_binned = np.zeros(num_bins)\n",
    "\n",
    "        # Bin the values and errors\n",
    "        for j, bin_values in enumerate(x_binned):\n",
    "            diff1_binned[j] = np.mean(diff1[bin_values.index])\n",
    "            diff2_binned[j] = np.mean(diff2[bin_values.index])\n",
    "            diff3_binned[j] = np.mean(diff3[bin_values.index])\n",
    "            error_diff1_binned[j] = np.sqrt(np.mean(error_diff1[bin_values.index]**2))\n",
    "            error_diff2_binned[j] = np.sqrt(np.mean(error_diff2[bin_values.index]**2))\n",
    "            error_diff3_binned[j] = np.sqrt(np.mean(error_diff3[bin_values.index]**2))\n",
    "\n",
    "        # Plot the scatter plot with error bars\n",
    "        plt.errorbar(np.arange(num_bins) * bin_size, diff1_binned, yerr=error_diff1_binned, markersize=5, fmt='.', label='T-C1')\n",
    "        plt.errorbar(np.arange(num_bins) * bin_size, diff2_binned, yerr=error_diff2_binned, markersize=5, fmt='.', label='T-C2')\n",
    "        plt.errorbar(np.arange(num_bins) * bin_size, diff3_binned, yerr=error_diff3_binned, markersize=3, fmt='.', label='C1-C2')\n",
    "        plt.axhline(y=np.mean(diff3_binned), color='r')\n",
    "        plt.ylabel('Differential magnitude')\n",
    "        plt.xlabel('JD')\n",
    "        plt.title(f'Difference of Magnitude {i}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print('Execution took:', end - start, 'seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12b9e4",
   "metadata": {},
   "source": [
    "### **Phase-Magnitude diagrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of metadata file names\n",
    "metadata_files = ['/home/aries/projetFolder/NGC6709/NGC6709_1/20211009/metadata.csv',\n",
    "             '/home/aries/projetFolder/NGC6709/NGC6709_2/20211105/metadata.csv', \n",
    "             '/home/aries/projetFolder/NGC6709/NGC6709_3/20211106/metadata.csv', \n",
    "             '/home/aries/projetFolder/NGC6709/NGC6709_4/20211116/metadata.csv', \n",
    "             '/home/aries/projetFolder/NGC6709/NGC6709_5/20211117/metadata.csv',\n",
    "            '/home/aries/projetFolder/NGC6709/NGC6709_6/20211120/metadata.csv',\n",
    "            '/home/aries/projetFolder/NGC6709/NGC6709_7/20220319/metadata.csv',\n",
    "            '/home/aries/projetFolder/NGC6709/NGC6709_8/20220328/metadata.csv',\n",
    "            '/home/aries/projetFolder/NGC6709/NGC6709_9/20220410/metadata.csv',\n",
    "            '/home/aries/projetFolder/NGC6709/NGC6709_12/20221031/metadata.csv']\n",
    "\n",
    "for k in range(1,20):\n",
    "    #k= row of metadata.csv\n",
    "    # Initialize an empty list to store the file names\n",
    "    RA =[]\n",
    "    DEC =[]\n",
    "    file_names = []\n",
    "    # Iterate over each metadata file\n",
    "    for metadata_file in metadata_files:\n",
    "        # Read the metadata.csv file\n",
    "        metadata_df = pd.read_csv(metadata_file)\n",
    "\n",
    "        # Get the file paths from the 3rd row under the 'file' column\n",
    "        file_paths = metadata_df.loc[k, 'File']\n",
    "        Ra, Dec = metadata_df.loc[k, 'RA'], metadata_df.loc[k, 'DEC']\n",
    "        # Split the file paths into a list\n",
    "        file_paths_list = file_paths.split(',')\n",
    "        #Ra_list, Dec_list = Ra.split(','), Dec.split(',')\n",
    "        # Append the third file path to the file_names list\n",
    "        file_names.append(file_paths_list)  # Assuming the third file path is required\n",
    "        RA.append(Ra) \n",
    "        DEC .append(Dec)\n",
    "    file_names = list(itertools.chain(*file_names))\n",
    "    print(RA, DEC)\n",
    "    # Function to perform sigma clipping for a given DataFrame\n",
    "    def perform_sigma_clipping(df):\n",
    "        mean = df['Calibrated_Magnitude'].mean()\n",
    "        std = df['Calibrated_Magnitude'].std()\n",
    "        cutoff = 3 * std\n",
    "        df = df[(df['Calibrated_Magnitude'] >= mean - cutoff) & (df['Calibrated_Magnitude'] <= mean + cutoff)]\n",
    "        return df\n",
    "    \n",
    "    combined_dfs = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_name)\n",
    "        df = perform_sigma_clipping(df)\n",
    "\n",
    "        combined_dfs.append(df)\n",
    "\n",
    "    # Concatenate all the DataFrames in the combined list\n",
    "    combined_df = pd.concat(combined_dfs)\n",
    "    print(type(combined_df['JD']))\n",
    "\n",
    "    x, y =(RA[0]), (DEC[0])\n",
    "    \n",
    "    # Plotting the data\n",
    "    plt.errorbar(combined_df['JD'], combined_df['Calibrated_Magnitude'],yerr=combined_df['Magnitude_error0'], markersize=5, fmt='.')\n",
    "    plt.xlabel('JD in days')\n",
    "    plt.ylabel('Calibrated Magnitude')\n",
    "    plt.title('Light curve for RA={} Dec={}'.format(x, y)) \n",
    "    #plt.xlim(2459497.1036500,2459497.17280000)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29de806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "jd = combined_df['JD']  \n",
    "magnitudes = combined_df['Calibrated_Magnitude'] \n",
    "mag_err = combined_df['Magnitude_error0']\n",
    "\n",
    "# Calculate the Lomb-Scargle periodogram\n",
    "frequency, power = LombScargle(jd, magnitudes).autopower()\n",
    "\n",
    "dominant_period = 1 / frequency[np.argmax(power)]\n",
    "phase = ((jd - jd[0]) / dominant_period) % 1\n",
    "\n",
    "amplitude = np.max(magnitudes) - np.min(magnitudes)\n",
    "dominant_period = round(dominant_period,5)\n",
    "amplitude = round(amplitude,5)\n",
    "\n",
    "# Plot the folded light curve\n",
    "plt.text(0.5,8,f\"P = {dominant_period} days\", fontsize =12) \n",
    "plt.text(0.5,8.1,f\"A = {amplitude}\", fontsize =12)\n",
    "\n",
    "plt.errorbar(phase, magnitudes, yerr=mag_err, markersize=1,fmt='.', color='black')\n",
    "plt.xlabel('Phase')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title(f'RA = {RA[0]}, DEC = {DEC[0]}')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"Period is\", dominant_period, 'days')\n",
    "print(\"Amplitude:\", amplitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891ea1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
